{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson-01 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 今天是2020年1月05日，今天世界上又多了一名AI工程师 :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`各位同学大家好，欢迎各位开始学习我们的人工智能课程。这门课程假设大家不具备机器学习和人工智能的知识，但是希望大家具备初级的Python编程能力。根据往期同学的实际反馈，我们课程的完结之后 能力能够超过80%的计算机人工智能/深度学习方向的硕士生的能力。`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本次作业的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 复现课堂代码\n",
    "\n",
    "在本部分，你需要参照我们给大家的GitHub地址里边的课堂代码，结合课堂内容，复现内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 作业截止时间\n",
    "此次作业截止时间为 2020.01.12日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 完成以下问答和编程练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础理论部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点**：每道题是否回答完整"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Can you come up out 3 sceneraies which use AI methods? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: {conversional robot, manless driving, face recognition}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. How do we use Github; Why do we use Jupyter and Pycharm;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: {cmd> path\n",
    "     git config -- global user.name \"yourname\"\n",
    "     git config -- global user,email \"youremail\"\n",
    "     git config --l\n",
    "     git init # create git project\n",
    "     git remote add origin \"your github repository URL\" #connect your github repository\n",
    "     git add filename # local add \n",
    "     git commit -m \"description\" #local commit\n",
    "     git push -u origin master #remote push        welcome to my instance https://github.com/youngi3/NLP_traning/tree/master \n",
    "     jupyter notebook is an interactive complier supporing makedown and varios format conversion\n",
    "     sorry I hasn't used Pycharm. But I will try it before long.\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What's the Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:Bayes Probability Model:$$ Pr(w_1 | w_2 ) =\\frac{Pro(w_1 w_2)}{P(w2 )}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Pr(w_1 | w_2 ) =\\frac{Pro(w_2 | w_1)*Pro(w_1)}{P(w2 )}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Can you came up with some sceneraies at which we could use Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:epidemic disease analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:To examine whether the generated grammar is true.\n",
    "The difficult is self-iteration program design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What's the Language Model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:1 - GRAM MODEL :$$ Pro(w_1 w_2 w_3 w_4) = Pr(w_1 | w_2 w_3 w_ 4) * P(w2 | w_3 w_4) * Pr(w_3 | w_4) * Pr(w_4)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Pro(w_1 w_2 w_3 w_4) \\sim Pr(w_1 | w_2 ) * P(w2 | w_3 ) * Pr(w_3 | w_4) * Pr(w_4)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Can you came up with some sceneraies at which we could use Language Model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:BERT, SQUAD ,arcticle tanslation ,conversitional robot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. What's the 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:$$ Pro(w_1 w_2 w_3 w_4) \\sim Pr(w_1 | w_2 ) * P(w2 | w_3 ) * Pr(w_3 | w_4) * Pr(w_4)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-gram language model assume that a word is just relate to the next word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. What's the disadvantages and advantages of 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:disadvantage:inadequate consideration about context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    advantage:not complicatied and saving time and memory space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. What't the 2-gram models;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:2 - GRAM MODEL :$$ Pro(w_1 w_2 w_3 w_4) = Pr(w_1 | w_2 w_3 w_ 4) * P(w2 | w_3 w_4) * Pr(w_3 w_4)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Pro(w_1 w_2 w_3 w_4) \\sim Pr(w_1 | w_2 w_3) * P(w2 | w_3 w_4) * Pr(w_3 w_4) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-gram language model assume that a word is just relate to the latter two word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编程实践部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 设计你自己的句子生成器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何生成句子是一个很经典的问题，从1940s开始，图灵提出机器智能的时候，就使用的是人类能不能流畅和计算机进行对话。和计算机对话的一个前提是，计算机能够生成语言。\n",
    "\n",
    "计算机如何能生成语言是一个经典但是又很复杂的问题。 我们课程上为大家介绍的是一种基于规则（Rule Based）的生成方法。该方法虽然提出的时间早，但是现在依然在很多地方能够大显身手。值得说明的是，现在很多很实用的算法，都是很久之前提出的，例如，二分查找提出与1940s, Dijstra算法提出于1960s 等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在著名的电视剧，电影《西部世界》中，这些机器人们语言生成的方法就是使用的SyntaxTree生成语言的方法。\n",
    "\n",
    "> \n",
    ">\n",
    "\n",
    "![WstWorld](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1569578233461&di=4adfa7597fb380e7cc0e67190bbd7605&imgtype=0&src=http%3A%2F%2Fs1.sinaimg.cn%2Flarge%2F006eYYfyzy76cmpG3Yb1f)\n",
    "\n",
    "> \n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这一部分，需要各位同学首先定义自己的语言。 大家可以先想一个应用场景，然后在这个场景下，定义语法。例如：\n",
    "\n",
    "在西部世界里，一个”人类“的语言可以定义为：\n",
    "``` \n",
    "human = \"\"\"\n",
    "human = 自己 寻找 活动\n",
    "自己 = 我 | 俺 | 我们 \n",
    "寻找 = 看看 | 找找 | 想找点\n",
    "活动 = 乐子 | 玩的\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "一个“接待员”的语言可以定义为\n",
    "```\n",
    "host = \"\"\"\n",
    "host = 寒暄 报数 询问 业务相关 结尾 \n",
    "报数 = 我是 数字 号 ,\n",
    "数字 = 单个数字 | 数字 单个数字 \n",
    "单个数字 = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \n",
    "寒暄 = 称谓 打招呼 | 打招呼\n",
    "称谓 = 人称 ,\n",
    "人称 = 先生 | 女士 | 小朋友\n",
    "打招呼 = 你好 | 您好 \n",
    "询问 = 请问你要 | 您需要\n",
    "业务相关 = 玩玩 具体业务\n",
    "玩玩 = 耍一耍 | 玩一玩\n",
    "具体业务 = 喝酒 | 打牌 | 打猎 | 赌博\n",
    "结尾 = 吗？\"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请定义你自己的语法: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "日常 = '''\n",
    "句子 =哪天 时间 谁 询问\n",
    "哪天=  null | 以后\n",
    "以后 = 今天 | 明天 | 后天\n",
    "时间 = 下午 | 晚上 | 白天\n",
    "谁 = 对象*\n",
    "对象* = 对象 | 对象 对象*\n",
    "对象 = 老师 | 师兄 | 师姐 \n",
    "询问 = 问 对象 意愿 活动\n",
    "意愿 = 想不想 | 去不去 | 要不要\n",
    "活动 =  状态 动作 内容 | 动作 内容\n",
    "状态 = 一起 | 立刻 \n",
    "动作 = 看 | 做 | 讨论\n",
    "内容 = 项目 | 实验 | 论文\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点**： 是否提出了和课程上区别较大的语法结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "点菜= '''\n",
    "句子 = 礼貌 询问 评价 转折 拜托 动作 \n",
    "礼貌 = 打招呼 | 谦辞 | 打招呼 谦辞\n",
    "打招呼 = 您好 | 你好\n",
    "谦辞 = 打扰一下， | 麻烦一下，\n",
    "询问 = 代词 种类 | 种类\n",
    "代词 = 这个 | 那个 \n",
    "种类 = 菜品*\n",
    "菜品* = 菜品 | 菜品 菜品*\n",
    "菜品 = 烤鱼 | 锅包肉 | 豆腐\n",
    "评价 = 怎么样 | 好吃么\n",
    "转折 = 那 | emm.... | 那就 | emm... 那就\n",
    "拜托 = 请 | 请您\n",
    "动作 = 上 菜品 | 点 菜品\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点**：是否和上一个语法区别比较大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 然后，使用自己之前定义的generate函数，使用此函数生成句子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grammar(grammar_str, split='=>', line_split='\\n'):\n",
    "    grammar = {}\n",
    "    for line in grammar_str.split(line_split):\n",
    "        if not line.strip(): continue\n",
    "        exp, stmt = line.split(split)\n",
    "        grammar[exp.strip()] = [s.split() for s in stmt.split('|')]\n",
    "    return grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "choice = random.choice\n",
    "\n",
    "def generate(gram, target):\n",
    "    if target not in gram: return target # means target is a terminal expression\n",
    "    \n",
    "    expaned = [generate(gram, t) for t in choice(gram[target])]\n",
    "    return ''.join([e if e != '/n' else '\\n' for e in expaned if e != 'null'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "白天师兄问师姐想不想立刻看实验\n",
      "后天晚上师姐问师姐要不要立刻做论文\n",
      "白天师姐老师问师姐要不要立刻讨论实验\n",
      "下午师姐师姐问师姐想不想讨论实验\n",
      "下午师姐师姐师兄问师兄去不去做论文\n",
      "下午师兄问老师去不去一起做实验\n",
      "晚上师姐师姐问师兄想不想看电影\n",
      "白天师姐问师姐要不要立刻做电影\n",
      "下午师姐问师姐要不要立刻做实验\n",
      "明天晚上师兄问老师去不去做电影\n",
      "晚上老师师姐问师姐想不想看实验\n",
      "后天晚上师姐老师师兄问师兄要不要立刻做实验\n",
      "白天老师老师问师兄要不要立刻看电影\n",
      "晚上师兄问师兄要不要立刻讨论实验\n",
      "白天老师问老师去不去做论文\n",
      "下午师兄老师老师问老师要不要一起做电影\n",
      "明天白天师兄问老师要不要立刻讨论电影\n",
      "今天下午老师师姐问师姐去不去看电影\n",
      "今天白天师姐问师姐要不要立刻讨论论文\n",
      "下午师兄问师姐要不要做实验\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(generate(gram=create_grammar(日常, split='='), target='句子'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "麻烦一下，那个锅包肉怎么样emm....请您点豆腐\n",
      "打扰一下，锅包肉豆腐好吃么emm....请点烤鱼\n",
      "您好打扰一下，这个锅包肉好吃么emm....请上锅包肉\n",
      "你好打扰一下，锅包肉锅包肉好吃么emm...那就请点豆腐\n",
      "麻烦一下，这个锅包肉好吃么那请您上烤鱼\n",
      "麻烦一下，锅包肉锅包肉烤鱼锅包肉烤鱼烤鱼锅包肉好吃么那请您上豆腐\n",
      "麻烦一下，锅包肉豆腐锅包肉锅包肉怎么样emm....请点烤鱼\n",
      "你好麻烦一下，这个锅包肉锅包肉烤鱼怎么样emm...那就请点豆腐\n",
      "你好那个锅包肉锅包肉豆腐怎么样那就请点豆腐\n",
      "您好打扰一下，这个豆腐锅包肉好吃么那就请您上豆腐\n",
      "你好这个烤鱼锅包肉怎么样那就请点烤鱼\n",
      "你好麻烦一下，这个烤鱼豆腐好吃么那就请点烤鱼\n",
      "麻烦一下，烤鱼锅包肉烤鱼烤鱼怎么样emm....请您上豆腐\n",
      "您好打扰一下，这个豆腐好吃么那请您上豆腐\n",
      "你好打扰一下，烤鱼烤鱼豆腐锅包肉豆腐怎么样emm....请点锅包肉\n",
      "打扰一下，锅包肉锅包肉好吃么emm...那就请上锅包肉\n",
      "您好烤鱼烤鱼怎么样emm....请您点豆腐\n",
      "您好那个豆腐烤鱼好吃么那请上烤鱼\n",
      "您好这个锅包肉豆腐怎么样那请上锅包肉\n",
      "您好麻烦一下，那个烤鱼烤鱼锅包肉烤鱼怎么样emm....请上豆腐\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(generate(gram=create_grammar(点菜, split='='), target='句子'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 然后，定义一个函数，generate_n，将generate扩展，使其能够生成n个句子:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n(n, need_gram, need_target):\n",
    "    for i in range (n):\n",
    "        print(generate(gram=create_grammar(need_gram, split='='), target=need_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "麻烦一下，豆腐怎么样那请上豆腐\n",
      "你好麻烦一下，那个豆腐豆腐烤鱼怎么样emm...那就请点豆腐\n",
      "您好烤鱼锅包肉怎么样emm...那就请您上烤鱼\n"
     ]
    }
   ],
   "source": [
    "generate_n(3,need_gram = 点菜, need_target = \"句子\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点**; 运行代码，观察是否能够生成多个句子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 使用新数据源完成语言模型的训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照我们上文中定义的`prob_2`函数，我们更换一个文本数据源，获得新的Language Model:\n",
    "\n",
    "1. 下载文本数据集（你可以在以下数据集中任选一个，也可以两个都使用）\n",
    "    + 可选数据集1，保险行业问询对话集： https://github.com/Computing-Intelligence/insuranceqa-corpus-zh/raw/release/corpus/pool/train.txt.gz\n",
    "    + 可选数据集2：豆瓣评论数据集：https://github.com/Computing-Intelligence/datasource/raw/master/movie_comments.csv\n",
    "2. 修改代码，获得新的**2-gram**语言模型\n",
    "    + 进行文本清洗，获得所有的纯文本\n",
    "    + 将这些文本进行切词\n",
    "    + 送入之前定义的语言模型中，判断文本的合理程度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点** 1. 是否使用了新的数据集； 2. csv(txt)数据是否正确解析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2—gram Language Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Pro(w_1 w_2 w_3 w_4) \\sim Pr(w_1 | w_2 w_3) * P(w2 | w_3 w_4) * Pr(w_3 w_4) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = r'C:\\Users\\aceryoung\\Downloads\\NLP\\02\\assignment\\Assignment-01\\movie_comments.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = pd.read_csv(filename, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京意淫到了脑残的地步，看了恶心想吐</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>中二得很</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                        link name  \\\n",
       "0  1  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "1  2  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "2  3  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "3  4  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "4  5  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "\n",
       "                                             comment star  \n",
       "0                                 吴京意淫到了脑残的地步，看了恶心想吐    1  \n",
       "1  首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...    2  \n",
       "2  吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...    2  \n",
       "3                      凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。    4  \n",
       "4                                               中二得很    1  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = content['comment'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261497"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(string):\n",
    "    # we will learn the regular expression next course.\n",
    "    return re.findall('\\w+', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_jieba_cut = Counter(jieba.cut(articles[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Tokenizer.cut at 0x0000014CBA623548>"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba.cut(articles[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('，', 5),\n",
       " ('的', 3),\n",
       " ('。', 2),\n",
       " ('这个', 2),\n",
       " ('首映礼', 1),\n",
       " ('看', 1),\n",
       " ('太', 1),\n",
       " ('恐怖', 1),\n",
       " ('了', 1),\n",
       " ('电影', 1)]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_jieba_cut.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['首映礼看的',\n",
       " '太恐怖了这个电影',\n",
       " '不讲道理的',\n",
       " '完全就是吴京在实现他这个小粉红的英雄梦',\n",
       " '各种装备轮番上场',\n",
       " '视物理逻辑于不顾',\n",
       " '不得不说有钱真好',\n",
       " '随意胡闹']"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token(articles[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'三星半实打实的7分第一集在爱国主旋律内部做着各种置换与较劲但第二集才真正显露吴京的野心他终于抛弃李忠志了新增外来班底让硬件实力有机会和国际接轨开篇水下长镜头和诸如铁丝网拦截RPG弹头的细节设计都让国产动作片重新封顶在理念上它甚至做到绣春刀2最想做到的那部分'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(token(articles[7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_clean = [''.join(token(str(a)))for a in articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261497"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('article_movie.txt', 'w', encoding = 'utf-8') as f:\n",
    "    for a in articles_clean:\n",
    "        f.write(a + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 驱动器 C 中的卷是 Acer\n",
      " 卷的序列号是 BA06-64FD\n",
      "\n",
      " C:\\Users\\aceryoung\\Downloads\\NLP\\02\\assignment\\Assignment-01 的目录\n",
      "\n",
      "2020-01-12  00:46    <DIR>          .\n",
      "2020-01-12  00:46    <DIR>          ..\n",
      "2020-01-10  20:36    <DIR>          .ipynb_checkpoints\n",
      "2020-01-12  00:46        23,511,075 article_movie.txt\n",
      "2020-01-12  00:46            60,060 Assignment-01.ipynb\n",
      "2020-01-11  23:40        46,833,745 movie_comments.csv\n",
      "2020-01-11  23:18         1,660,053 train.txt\n",
      "               4 个文件     72,064,933 字节\n",
      "               3 个目录 47,847,727,104 可用字节\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(string): return list(jieba.cut(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n"
     ]
    }
   ],
   "source": [
    "for i, line in enumerate((open('article_movie.txt', encoding = 'utf-8'))):\n",
    "    if i % 10000 == 0: print(i)\n",
    "    \n",
    "    # replace 10000 with a big number when you do your homework. \n",
    "    \n",
    "    #if i > 10000: break    \n",
    "    TOKEN += cut(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_count = Counter(TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('的', 328262),\n",
       " ('\\n', 261497),\n",
       " ('了', 102420),\n",
       " ('是', 73106),\n",
       " ('我', 50338),\n",
       " ('都', 36255),\n",
       " ('很', 34712),\n",
       " ('看', 34022),\n",
       " ('电影', 33675),\n",
       " ('也', 32065)]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_count.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequiences = [f for w, f in words_count.most_common(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [i for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14cb7129a08>]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, frequiences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_2_GRAM = [''.join(TOKEN[i:i+2]) for i in range(len(TOKEN[:-2]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['吴京意淫', '意淫到', '到了', '了脑残', '脑残的', '的地步', '地步看', '看了', '了恶心', '恶心想']"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKEN_2_GRAM[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4751808"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TOKEN_2_GRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_count_2 = Counter(TOKEN_2_GRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1678239"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_count_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_1(word):\n",
    "    return words_count[word] / len(TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_2(word1, word2):\n",
    "    if word1 + word2 in words_count_2: return words_count_2[word1+word2] / words_count[word2]\n",
    "    else:\n",
    "        return 1 / len(TOKEN_2_GRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.85040226776744e-05"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_1('吴京')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015590200445434299"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_2('看', '电影')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probablity(sentence):\n",
    "    words = cut(sentence)\n",
    "    \n",
    "    sentence_pro = 1\n",
    "    \n",
    "    for i, word in enumerate(words[:-1]):\n",
    "        next_ = words[i+1]\n",
    "        \n",
    "        probability = prob_2(word, next_)\n",
    "        \n",
    "        sentence_pro *= probability\n",
    "    sentence_pro *= prob_1(words[-1]) \n",
    "    \n",
    "    return sentence_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.343266871404879e-40"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_probablity('小明今天抽奖抽到一台苹果手机')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好锅包肉好吃么那就请点锅包肉\n"
     ]
    }
   ],
   "source": [
    "generate_n(1,need_gram =点菜, need_target = \"句子\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sen_pro(n, your_str, your_tar):\n",
    "    for sen in [generate(gram=create_grammar(your_str, split='='), target=your_tar) for i in range(n)]:\n",
    "        print('sentence: {} with Prb: {}'.format(sen, get_probablity(sen)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: 下午师姐问老师去不去立刻讨论论文 with Prb: 1.2695291642516933e-52\n",
      "sentence: 晚上老师师姐老师问师姐想不想看论文 with Prb: 3.26774358270107e-52\n",
      "sentence: 晚上师兄问师姐要不要做实验 with Prb: 2.6755972986748714e-37\n",
      "sentence: 白天师姐问老师要不要做实验 with Prb: 7.866554045676058e-34\n",
      "sentence: 后天白天师姐师兄问师兄要不要做实验 with Prb: 1.184958061988222e-50\n",
      "sentence: 今天晚上师兄老师师姐问师兄要不要立刻做实验 with Prb: 3.0273156333410057e-63\n",
      "sentence: 后天下午师兄问师姐去不去一起做项目 with Prb: 9.113314961741085e-50\n",
      "sentence: 下午师兄师兄老师问师姐要不要立刻看项目 with Prb: 2.5993154151075348e-67\n",
      "sentence: 白天师兄师姐老师问老师去不去讨论实验 with Prb: 5.880503055191826e-53\n",
      "sentence: 下午老师问老师想不想立刻讨论实验 with Prb: 4.99774814779792e-48\n"
     ]
    }
   ],
   "source": [
    "sen_pro(10, 日常, '句子')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天晚上请你看电影，我们一起看战狼 is more possible\n",
      "---- 今天晚上请你看电影，我们一起看战狼 with probility 5.2835608988841897e-48\n",
      "---- 明天晚上请你看电影，我们一起看红海行动 with probility 8.006516532146899e-51\n",
      "真是一只好看的小猫 is more possible\n",
      "---- 真事一只好看的小猫 with probility 1.671538644906592e-24\n",
      "---- 真是一只好看的小猫 with probility 3.47684307953857e-18\n",
      "今晚我去吃火锅 is more possible\n",
      "---- 今晚我去吃火锅 with probility 2.2694199384055505e-13\n",
      "---- 今晚火锅去吃我 with probility 7.338705216236832e-22\n",
      "养乐多绿来一杯 is more possible\n",
      "---- 洋葱奶昔来一杯 with probility 9.320155624620777e-21\n",
      "---- 养乐多绿来一杯 with probility 6.803713605973167e-19\n"
     ]
    }
   ],
   "source": [
    "need_compared = [\n",
    "    \"今天晚上请你看电影，我们一起看战狼 明天晚上请你看电影，我们一起看红海行动\",\n",
    "    \"真事一只好看的小猫 真是一只好看的小猫\",\n",
    "    \"今晚我去吃火锅 今晚火锅去吃我\",\n",
    "    \"洋葱奶昔来一杯 养乐多绿来一杯\"\n",
    "]\n",
    "\n",
    "for s in need_compared:\n",
    "    s1, s2 = s.split()\n",
    "    p1, p2 = get_probablity(s1), get_probablity(s2)\n",
    "    \n",
    "    better = s1 if p1 > p2 else s2\n",
    "    \n",
    "    print('{} is more possible'.format(better))\n",
    "    print('-'*4 + ' {} with probility {}'.format(s1, p1))\n",
    "    print('-'*4 + ' {} with probility {}'.format(s2, p2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 获得最优质的的语言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们能够生成随机的语言并且能判断之后，我们就可以生成更加合理的语言了。请定义 generate_best 函数，该函数输入一个语法 + 语言模型，能够生成**n**个句子，并能选择一个最合理的句子: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示，要实现这个函数，你需要Python的sorted函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 5]"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([1, 3, 5, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个函数接受一个参数key，这个参数接受一个函数作为输入，例如"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4), (2, 5), (4, 4), (5, 0)]"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第0个元素进行排序."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 0), (1, 4), (4, 4), (2, 5)]"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第1个元素进行排序."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 5), (1, 4), (4, 4), (5, 0)]"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第1个元素进行排序, 但是是递减的顺序。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_best(n,give_str,give_target):\n",
    "    a_list = []\n",
    "    for sen in [generate(gram=create_grammar(give_str, split='='), target=give_target) for i in range(n)]:\n",
    "        print('sentence: {} with Prb: {}'.format(sen, get_probablity(sen)))\n",
    "        tup = sen,get_probablity(sen)\n",
    "        a_list.append(tup)\n",
    "    print('最合理的句子和最大概率: {}'.format(sorted(a_list, key = lambda x : x[1], reverse = True)[0]))\n",
    "    #return sorted(a_list, key = lambda x : x[1], reverse = True)[0]\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: 今天白天师姐师兄老师问老师去不去看项目 with Prb: 9.518532760789219e-64\n",
      "sentence: 下午师姐老师老师问师兄去不去看论文 with Prb: 2.5031478313368626e-48\n",
      "sentence: 下午师姐师姐师姐师姐问老师去不去立刻讨论论文 with Prb: 1.1832214360907853e-72\n",
      "sentence: 今天晚上老师师兄问师兄想不想立刻看实验 with Prb: 6.932938937549036e-63\n",
      "sentence: 白天师兄师兄问师兄去不去一起看实验 with Prb: 5.251004726845208e-54\n",
      "sentence: 今天晚上老师问师姐去不去讨论项目 with Prb: 1.4337632415744527e-47\n",
      "sentence: 后天下午师兄问师兄要不要看项目 with Prb: 6.967784670187399e-50\n",
      "sentence: 晚上师兄问老师要不要讨论实验 with Prb: 2.193427094129965e-43\n",
      "sentence: 明天晚上师姐师姐问师兄想不想一起讨论论文 with Prb: 3.997494071994785e-59\n",
      "sentence: 明天晚上老师老师问师兄去不去一起做论文 with Prb: 3.329851821739407e-48\n",
      "sentence: 今天下午老师师兄问师兄想不想一起讨论实验 with Prb: 1.6526003902178434e-56\n",
      "sentence: 明天下午老师师姐问老师去不去立刻看项目 with Prb: 3.174897270466381e-62\n",
      "sentence: 白天老师问师姐去不去讨论项目 with Prb: 7.312192532029709e-46\n",
      "sentence: 白天师兄老师老师师兄问老师想不想立刻讨论实验 with Prb: 4.657981011521793e-68\n",
      "sentence: 晚上师姐问师兄想不想做项目 with Prb: 8.399009343357277e-39\n",
      "sentence: 下午师姐师姐问师兄要不要讨论论文 with Prb: 4.842065942318734e-54\n",
      "sentence: 白天师姐问老师想不想讨论项目 with Prb: 1.9056834058893e-45\n",
      "sentence: 晚上师姐问师姐要不要一起讨论项目 with Prb: 2.743197339044733e-49\n",
      "sentence: 下午老师师姐老师师兄问师兄想不想看实验 with Prb: 7.21394275959173e-60\n",
      "sentence: 明天晚上师姐师姐问老师去不去一起做实验 with Prb: 1.7869245023021737e-51\n",
      "最合理的句子和最大概率: ('晚上师姐问师兄想不想做项目', 8.399009343357277e-39)\n"
     ]
    }
   ],
   "source": [
    "generate_best(20,日常,\"句子\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好了，现在我们实现了自己的第一个AI模型，这个模型能够生成比较接近于人类的语言。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点**： 是否使用 lambda 语法进行排序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 这个模型有什么问题？ 你准备如何提升？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:该模型概率模型用到的词库为豆瓣影评数据，而生成的句子可能与影评无关系，超出概率模型的词库了\n",
    "   该模型用到的概率模型为1-gram,没有用到2-gram,甚至3-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**评阅点**: 是否提出了比较实际的问题，例如OOV问题，例如数据量，例如变成 3-gram问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 以下内容为可选部分，对于绝大多数同学，能完成以上的项目已经很优秀了，下边的内容如果你还有精力可以试试，但不是必须的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. (Optional) 完成基于Pattern Match的语句问答\n",
    "> 我们的GitHub仓库中，有一个assignment-01-optional-pattern-match，这个难度较大，感兴趣的同学可以挑战一下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 5. (Optional) 完成阿兰图灵机器智能原始论文的阅读\n",
    "1. 请阅读阿兰图灵关于机器智能的原始论文：https://github.com/Computing-Intelligence/References/blob/master/AI%20%26%20Machine%20Learning/Computer%20Machinery%20and%20Intelligence.pdf \n",
    "2. 并按照GitHub仓库中的论文阅读模板，填写完毕后发送给我: mqgao@kaikeba.com 谢谢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各位同学，我们已经完成了自己的第一个AI模型，大家对人工智能可能已经有了一些感觉，人工智能的核心就是，我们如何设计一个模型、程序，在外部的输入变化的时候，我们的程序不变，依然能够解决问题。人工智能是一个很大的领域，目前大家所熟知的深度学习只是其中一小部分，之后也肯定会有更多的方法提出来，但是大家知道人工智能的目标，就知道了之后进步的方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，希望大家对AI不要有恐惧感，这个并不难，大家加油！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1561828422005&di=48d19c16afb6acc9180183a6116088ac&imgtype=0&src=http%3A%2F%2Fb-ssl.duitang.com%2Fuploads%2Fitem%2F201807%2F28%2F20180728150843_BECNF.thumb.224_0.jpeg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
